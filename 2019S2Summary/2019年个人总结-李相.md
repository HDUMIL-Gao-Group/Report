<!--
 * @Description: 
 * @Author: shaonianruntu
 * @Github: 
 * @Date: 2020-01-17 10:54:53
 * @LastEditTime : 2020-01-23 21:57:19
 -->
# 2019年个人总结-李相

<a href="https://github.com/HDUMIL-Gao-Group"><img src="https://img.shields.io/badge/Organization-%20Gao%20Group%20@%20HDUMIL-blue"></img></a>
<a href="https://github.com/Ausiden"><img src="https://img.shields.io/badge/Auther-李相-yellow"></img></a>
<img src="https://img.shields.io/badge/Grade-研一-ff85c0"></img>

## 目录
- [2019年个人总结-李相](#2019%e5%b9%b4%e4%b8%aa%e4%ba%ba%e6%80%bb%e7%bb%93-%e6%9d%8e%e7%9b%b8)
  - [目录](#%e7%9b%ae%e5%bd%95)
  - [这学期主要工作](#%e8%bf%99%e5%ad%a6%e6%9c%9f%e4%b8%bb%e8%a6%81%e5%b7%a5%e4%bd%9c)
    - [数据预处理](#%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86)
    - [损失函数](#%e6%8d%9f%e5%a4%b1%e5%87%bd%e6%95%b0)
    - [目前得到的结果](#%e7%9b%ae%e5%89%8d%e5%be%97%e5%88%b0%e7%9a%84%e7%bb%93%e6%9e%9c)
  - [下学期准备做的事情：](#%e4%b8%8b%e5%ad%a6%e6%9c%9f%e5%87%86%e5%a4%87%e5%81%9a%e7%9a%84%e4%ba%8b%e6%83%85)

## 这学期主要工作

在CUHK数据集上以pix2pix为baseline，加入APDrawingGan中的local网络以及超分辨率重建论文中的广义梯度损失

### 数据预处理

在CUHK数据集用mtcnn实现人脸观测点检测，对于检测不到的人脸，在face++上进行检测，然后手动定位，效果图如下

<div align=center>
<img src="lx-assets/1.png"/>
</div>

图像的mask是通过朱师姐的parsing跑的，效果如下:

<div align=center>
<img src="lx-assets/2.png"/>
</div>

由于CUHK是200 * 250的图像,首先对图像进行填充至256 * 256，在训练的时候将他填充到286 * 286，测试时不变，此时baseline跑出来的fid是28左右，但是若测试时也是256 * 256，则跑出来的fid 是50多，可能是由于过拟合了，因为训练时填充286 * 286再随机裁剪到256 * 256,这个随机过程不容易导致过拟合

### 损失函数

用了GAN损失，L1损失、Ggrad（广泛梯度损失）以及Local损失

### 目前得到的结果

从左到右分别为

真实照片

真实的素描图像

baseline  : Gloss = CE_loss +lamb_L1 *  L1

baseline+glad : Gloss =CE_loss+lamb_L1 * L1 + lamb_glad * glad

baseline+local : Gloss = CE_loss + lamb_L1 * L1 + lamb_local * local

baseline+local+glad : Gloss = CE_loss + lamb_L1 * L1 +  lamb_glad * glad+lamb_local * local

得到的效果图：

<div align=center>
<img src="lx-assets/3.jpg"/>
<div align=center>

<div align=center>
<img src="lx-assets/4.jpg"/>
<div align=center>

<div align=center>
<img src="lx-assets/5.jpg"/>
<div align=center>

<div align=center>
<img src="lx-assets/6.jpg"/>
<div align=center>

实验的fid效果

baseline  : Gloss = CE_loss +lamb_L1 *  L1(100)

<div align=center>
<img src="lx-assets/7.png"/>
</div>

平均fid:28.43

baseline+glad : Gloss =CE_loss+lamb_L1 * L1（100） + lamb_glad * glad（100）

<div align=center>
<img src="lx-assets/8.png"/>
</div>

平均fid:28.00

baseline+local : Gloss = CE_loss + lamb_L1 * L1（100） + lamb_local * local（25）

<div align=center>
<img src="lx-assets/9.png"/>
</div>

平均fid:30.88

baseline+local+glad : Gloss = CE_loss + lamb_L1 * L1（100） +  lamb_glad（100）* glad+lamb_local（25） * local

<div align=center>
<img src="lx-assets/10.png"/>
</div>

平均fid:32.08

## 下学期准备做的事情：

1. 对于有些照片特别是XM2VTS0类型的照片容易照成分区域特别明显，可能是需要进一步调参，由于代码跑的比较慢，到目前为止还没有测出比较好的参数
2. 对融合网络进行修改，看哪种效果比较好
3. 区域生成存在的问题是照片x和画像y各个区域之间并没有精确对齐，训练时会引入误差。方法：(1) 学习 照片parsing Px 和画像parsing Py 之间的几何变形关系：Px = T(Py)。T应该是个坐标之间的变换。(2) 然后，把变换关系T应用到对应的画像上，对画像进行校正：y' = T(y)。利用y'进行学习。将这个操作可以用到整张人脸上，也可以考虑在各个区域单独使用。（参考时尚相关的论文）目的是：找到一个不是通过训练的模型，而是给出 Px，Py 可以自动估计两者之间的坐标映射关系。
1. 在寒假时需要对挑战杯的文本进行修改



