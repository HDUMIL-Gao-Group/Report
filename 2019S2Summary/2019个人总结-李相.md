总结

#### 主要工作

在CUHK数据集上以pix2pix为baseline，加入APDrawingGan中的local网络以及超分辨率重建论文中的广义梯度损失

##### 数据预处理

在CUHK数据集用mtcnn实现人脸观测点检测，对于检测不到的人脸，在face++上进行检测，然后手动定位，效果图如下

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/1.png )

图像的mask是通过朱师姐的parsing跑的，效果如下:

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/2.jpg)



由于CUHK是200 * 250的图像,首先对图像进行填充至256 * 256，在训练的时候将他填充到286 * 286，测试时不变，此时baseline跑出来的fid是28左右，但是若测试时也是256 * 256，则跑出来的fid 是50多，可能是由于过拟合了，因为训练时填充286 * 286再随机裁剪到256 * 256,这个随机过程不容易导致过拟合

##### 损失函数

用了GAN损失，L1损失、Ggrad（广泛梯度损失）以及Local损失

##### 目前得到的结果

从左到右分别为

真实照片

真实的素描图像

baseline  : Gloss = CE_loss +lamb_L1 *  L1

baseline+glad : Gloss =CE_loss+lamb_L1 * L1 + lamb_glad * glad

baseline+local : Gloss = CE_loss + lamb_L1 * L1 + lamb_local * local

baseline+local+glad : Gloss = CE_loss + lamb_L1 * L1 +  lamb_glad * glad+lamb_local * local

得到的效果图：

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/3.jpg)

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/4.jpg)

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/5.jpg)

![image-20191109104352401](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/6.jpg)

实验的fid效果

baseline  : Gloss = CE_loss +lamb_L1 *  L1(100)

![image-20200113214425233](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/7.png)

平均fid:28.43

baseline+glad : Gloss =CE_loss+lamb_L1 * L1（100） + lamb_glad * glad（100）

![image-20200113214727821](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/8.png)

平均fid:28.00

baseline+local : Gloss = CE_loss + lamb_L1 * L1（100） + lamb_local * local（25）

![image-20200113214727821](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/9.png)

平均fid:30.88

baseline+local+glad : Gloss = CE_loss + lamb_L1 * L1（100） +  lamb_glad（100）* glad+lamb_local（25） * local

![image-20200115155916856](https://github.com/HDUMIL-Gao-Group/Report/blob/master/2019S2Summary/dln-assets/10.png)

平均fid:32.08

#### 下学期准备做的事情：

1.对于有些照片特别是XM2VTS0类型的照片容易照成分区域特别明显，可能是需要进一步调参，由于代码跑的比较慢，到目前为止还没有测出比较好的参数

2.对融合网络进行修改，看哪种效果比较好

3.区域生成存在的问题是照片x和画像y各个区域之间并没有精确对齐，训练时会引入误差。
方法：
1）学习 照片parsing Px 和画像parsing Py 之间的几何变形关系：Px = T(Py)。T应该是个坐标之间的变换。
2）然后，把变换关系T应用到对应的画像上，对画像进行校正：y' = T(y)。利用y'进行学习。

将这个操作可以用到整张人脸上，也可以考虑在各个区域单独使用。（参考时尚相关的论文）

目的是：找到一个不是通过训练的模型，而是给出 Px，Py 可以自动估计两者之间的坐标映射关系。

4.在寒假时需要对挑战杯的文本进行修改



